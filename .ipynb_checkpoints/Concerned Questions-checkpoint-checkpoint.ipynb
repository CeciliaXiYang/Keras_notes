{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concerned Questions of Keras\n",
    "\n",
    "## ** [Keras Github Docs](https://github.com/fchollet/keras)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed length of sequences:\n",
    "\n",
    "### 1) How to solve mixed lengths of sequences?\n",
    "\n",
    "a) Pad & truncate sequences to be the same length: In sequence preprocessing step, conduct **pad_sequences**. \n",
    "  \n",
    "   - Converted num_timesteps either indicated by **max_len** argument, or the length of longest sequence. \n",
    "  \n",
    "   - Position of padding or truncting is indicted like: **padding='pre'**, **truncating='post'**\n",
    "  \n",
    "   - **value=0.** in argument indicates the value to be padded. (or pad with neutual data)\n",
    "\n",
    "\n",
    "b) Group sequences into batches by length, i.e. same length of sequences grouped into same batch.\n",
    "\n",
    "\n",
    "c) Set batch size as 1. [Recurrent Models with sequences of mixed length](https://github.com/fchollet/keras/issues/40)\n",
    "\n",
    "\n",
    "d) Use a masking layer\n",
    "\n",
    "   - Q: masking layer among different batch??\n",
    "\n",
    "e) Set sample_weight parameter\n",
    "\n",
    "\n",
    "### 2) Difference between <font style=\"color:green\">*Masking layer*</font> and <font style=\"color: green\">*Sample_weights*</font>?\n",
    "\n",
    "**Sample_weight is a hand defined mask.** [Using Masking Layer for Sequence to Sequence Learning](https://github.com/fchollet/keras/issues/957)\n",
    "\n",
    "a) Sample_weight:\n",
    "\n",
    "   - List or numpy array with 1:1 mapping to the training samples, used for scaling the loss function during training only.\n",
    "  \n",
    "   - For time-distributed data, there is one weight per sample **per timestep**, i.e. if output data is shaped (nb_sample, timesteps, output_dim), the mask should be of shape (nb_sample, timesteps). \n",
    "  \n",
    "   - **sampe_weight allows to mask out or reweight individual output timesteps in sequence to sequence learning.** \n",
    "   [sample_weight docs](https://github.com/fchollet/keras/pull/494/commits/73fdaf6d6f8cd4de98db79ae93638d300b8de2b5)\n",
    "  \n",
    "   - Need to specify **sample_weight_mode=\"temporal\"** in compile().\n",
    "  \n",
    "   - For validation data, passed as a part of validation_data tuple:\n",
    "  \n",
    "  [Set sample_weight in validation](https://github.com/fchollet/keras/issues/496)\n",
    "  \n",
    "  [Optionally mask cost function for sequence to sequence learning](https://github.com/fchollet/keras/pull/451)\n",
    "  \n",
    "  [Is the sequence to sequence learning right?](https://github.com/fchollet/keras/issues/395)\n",
    "  \n",
    "b) Masking layer:\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "[Does masking only work for homogeneous batches?](https://github.com/fchollet/keras/issues/1206)\n",
    "\n",
    "[How does Masking work?](https://github.com/fchollet/keras/issues/3086)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to many rnn\n",
    "\n",
    "1) **Set return_sequence=True, and then make sure to warp Dense with a TimeDistributed wrapper layer.**\n",
    "\n",
    "2) Return_sequence\n",
    "\n",
    "   - return_sequence=True: Information transfered not only to the next layer, but also to the next timestep.\n",
    "\n",
    "   - return_sequence=False: For input 0 to seq_len-2, the prediction only passed to the layer itself for the next timestep and not as input to the next layer. Only the seq_len-1 input is passed forward to the dense layer for the loss computation against the target.\n",
    "\n",
    "References:\n",
    "\n",
    "[**Andrej Karpathy blog**: The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "[LSTM many to many mapping problem](https://github.com/fchollet/keras/issues/2403)\n",
    "\n",
    "** A very detailed explaination:** [Keras recurrent tutorial](https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation of using batch_size?\n",
    "\n",
    "### 1) Concepts of main parameters:\n",
    "\n",
    "1) **Epoch**: One forward pass and backward pass of **ALL** training samples.\n",
    "\n",
    "2) **Batch_size**: The number of training samples in one forward and backward pass. The larger batch size, the more memory required.\n",
    "\n",
    "3) **Iteration**: Number of passes for passing batch size of samples.\n",
    "\n",
    "### 2) Advantage and disadvantage of using batch_size?\n",
    "\n",
    "1) Advantage: \n",
    "\n",
    "   - Less memory required.\n",
    "   - Trains faster with mini_batch ?? (Update paramter after each Iteration)\n",
    "\n",
    "2) Disadvantage:\n",
    "   \n",
    "   The smaller the batch size, the less of the accuracy. (Model tend to be stochestic)\n",
    "\n",
    "Reference:\n",
    "\n",
    "[What is batch size in neural network?](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization methods in keras?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does stateful or not stateful mean in rnn? \n",
    "\n",
    "If argument **stateful=True**, the last state for each sample at index **i** in a batch will be utilized as initial state for the sample of index **i** in the following batch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction resuslts of rnn\n",
    "\n",
    "1) evaluate: returns list of scalars/metrics, with the metrics indicted by **`model.metrics_names`**.\n",
    "\n",
    "2) predict\n",
    "\n",
    "3) predict_class\n",
    "\n",
    "Reference:\n",
    "\n",
    "[**Brownlee Bolg**: Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
